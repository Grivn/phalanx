package txpool

import (
	"container/list"
	"fmt"
	"sync"
	"sync/atomic"

	commonProto "github.com/Grivn/phalanx/common/protos"
	commonTypes "github.com/Grivn/phalanx/common/types"
	"github.com/Grivn/phalanx/external"
	"github.com/Grivn/phalanx/internal"
	"github.com/Grivn/phalanx/timer"
)

// txPoolImpl is the implementation of txPool
type txPoolImpl struct {
	// mutex is used to process concurrent problems
	mutex sync.Mutex

	// author is the current node's identifier
	author uint64

	// isFull indicates if the txPool full or not
	isFull uint32

	// txPool would generate a batch while the number of txs has reached batchSize
	batchSize int

	// poolSize indicates the maximum size of txPool
	poolSize int

	// pendingTxs is used to record the transactions send from internal
	pendingTxs *recorder

	// batchStore is used to record the batches generated by self-node and send from others
	batchStore map[string]batchEntry

	// sendC is the channel group which is used to send back information to other modules
	sendC commonTypes.TxPoolSendChan

	// timeoutC is used to send the signals for timeout processor (timer)
	timeoutC chan interface{}

	// closeC is used to close the go-routine of txPool
	closeC chan bool

	// blockList is used to track the blocks waiting for execution
	blockList *list.List

	// pendingBlock is the front block in blockList which will be executed at first
	pendingBlock *pendingBlock

	// executor is used to execute blocks
	executor external.Executor

	// timer is used to process timeout events
	timer internal.Timer

	// logger is used to print logs
	logger external.Logger
}

// batchEntry is used to record batch with local target
type batchEntry struct {
	batch *commonProto.TxBatch
	local bool
}

// pendingBlock is the front block in blockList which will be executed at first
type pendingBlock struct {
	// e indicates the element pointer of current block in block list
	e *list.Element

	// blk contains the block information of current block
	blk *commonTypes.Block

	// endIndex is used to note the first log in current block that the batch of log is missed
	endIndex int

	// txList is the txs in current block
	txList []*commonProto.Transaction

	// localList indicates the txs from local or remote
	localList []bool
}

func newTxPoolImpl(author uint64, batchSize, poolSize int, sendC commonTypes.TxPoolSendChan, executor external.Executor, logger external.Logger) *txPoolImpl {
	timeoutC := make(chan interface{})
	timerService := timer.NewTimer(timeoutC, logger)

	return &txPoolImpl{
		author:     author,
		batchSize:  batchSize,
		poolSize:   poolSize,
		blockList:  list.New(),
		pendingTxs: newRecorder(),
		batchStore: make(map[string]batchEntry),

		sendC:    sendC,
		timeoutC: timeoutC,
		closeC:   make(chan bool),

		executor:   executor,
		timer:      timerService,
		logger:     logger,
	}
}

func (tp *txPoolImpl) start() {
	go tp.listener()
}

func (tp *txPoolImpl) stop() {
	select {
	case <-tp.closeC:
	default:
		close(tp.closeC)
	}
}

func (tp *txPoolImpl) isPoolFull() bool {
	return atomic.LoadUint32(&tp.isFull) == 1
}

func (tp *txPoolImpl) reset() {
	tp.pendingTxs.reset()
	tp.batchStore = make(map[string]batchEntry)
}

func (tp *txPoolImpl) listener() {
	for {
		select {
		case <-tp.closeC:
			tp.logger.Critical("exit tx pool listener")
			return
		case <-tp.timeoutC:
			tp.processTimeoutEvent()
		}
	}
}

//===================================
//       event processor
//===================================

func (tp *txPoolImpl) receiveTransactions(txs []*commonProto.Transaction) {
	tp.mutex.Lock()
	defer tp.mutex.Unlock()

	for _, tx := range txs {
		tp.pendingTxs.update(tx)

		// while the length of pending txs has reached batch size, txPool will generate a batch immediately
		if tp.pendingTxs.len() == tp.batchSize {
			// txPool is trying to generate a batch, stop the txPool timer
			tp.timer.StopTimer(commonTypes.TxPoolTimer)

			// generate a batch and post it out
			batch := tp.generateBatch()
			go tp.sendGeneratedBatch(batch)
		}

		if len(tp.pendingTxs.txList) > 0 {
			// there are some pending txs, txPool starts a timer to generate a batch after the interval of timer
			tp.timer.StartTimer(commonTypes.TxPoolTimer, true)
		}
	}

	tp.checkSpace()
}

func (tp *txPoolImpl) receiveTxBatch(batch *commonProto.TxBatch) error {
	tp.mutex.Lock()
	defer tp.mutex.Unlock()

	tp.logger.Infof("replica %d received batch %s from replica %d", tp.author, batch.Digest)

	// verify the correctness of batch
	if err := tp.verifyBatch(batch); err != nil {
		return fmt.Errorf("replica %d received illegal batch: %s", tp.author, err)
	}

	// record the batch with a false local target
	tp.batchStore[batch.Digest] = batchEntry{batch: batch, local: false}
	tp.checkSpace()

	return nil
}

func (tp *txPoolImpl) tryingBlockExecution(blk *commonTypes.Block) {
	tp.mutex.Lock()
	defer tp.mutex.Unlock()

	tp.blockList.PushBack(blk)

	if tp.blockList.Len() == 0 {
		// there isn't any block waiting for execution, just return
		return
	}

	// there are some blocks waiting for execution, but we haven't get the information of the first one to execute
	// here, we would like to find the information of the first block to execute
	if tp.pendingBlock == nil {
		e := tp.blockList.Front()
		blk, ok := e.Value.(*commonTypes.Block)
		if !ok {
			tp.logger.Error("parsing block type error")
			return
		}

		tp.pendingBlock = &pendingBlock{
			e:   e,
			blk: blk,
		}
	}

	blk = tp.pendingBlock.blk
	for i:= tp.pendingBlock.endIndex; i< len(blk.Logs); i++ {
		// get the first log in which the batch information hasn't been read
		log := blk.Logs[tp.pendingBlock.endIndex]

		entry, ok := tp.batchStore[log.ID]
		if !ok {
			// we cannot find the batch information just now
			tp.logger.Debugf("replica %d hasn't received batch %v for block %d", tp.author, log.ID, blk.Sequence)
			return
		}

		for _, tx := range entry.batch.TxList {
			tp.pendingBlock.txList = append(tp.pendingBlock.txList, tx)
			tp.pendingBlock.localList = append(tp.pendingBlock.localList, entry.local)
		}
		tp.pendingBlock.endIndex++
	}

	tp.logger.Noticef("======== replica %d call execute, seqNo=%d, timestamp=%d", tp.author, blk.Sequence, blk.Timestamp)
	tp.executor.Execute(tp.pendingBlock.txList, tp.pendingBlock.localList, blk.Sequence, blk.Timestamp)

	// remove the stored batchStore
	for _, log := range blk.Logs {
		delete(tp.batchStore, log.ID)
	}

	// remove the executed block
	tp.blockList.Remove(tp.pendingBlock.e)
	tp.pendingBlock = nil

	tp.checkSpace()
}

func (tp *txPoolImpl) processTimeoutEvent() {
	tp.mutex.Lock()
	defer tp.mutex.Unlock()

	if tp.pendingTxs.len() > 0 {
		batch := tp.generateBatch()
		go tp.sendGeneratedBatch(batch)
	}
}

//===================================
//       essential tools
//===================================

func (tp *txPoolImpl) generateBatch() *commonProto.TxBatch {
	batch := &commonProto.TxBatch{
		HashList:  tp.pendingTxs.hashes(),
		TxList:    tp.pendingTxs.txs(),
	}

	// calculate the digest for current batch
	hash := commonTypes.CalculateListHash(batch.HashList, 0)
	batch.Digest = hash

	// reset the pending txs and record the generated batch
	tp.pendingTxs.reset()
	tp.batchStore[hash] = batchEntry{batch: batch, local: true}

	tp.logger.Noticef("replica %d generate a tx-batch %s, len %d", tp.author, batch.Digest, len(batch.TxList))
	return batch
}

func (tp *txPoolImpl) verifyBatch(batch *commonProto.TxBatch) error {
	var hashList []string
	for _, tx := range batch.TxList {
		hashList = append(hashList, commonTypes.GetHash(tx))
	}

	// verify the correctness of batch digest
	if digest := commonTypes.CalculateListHash(hashList, 0); digest != batch.Digest {
		return fmt.Errorf("replica %d received a batch with mis-matched digest", tp.author)
	}
	return nil
}

//===================================
//       send back messages
//===================================
func (tp *txPoolImpl) sendGeneratedBatch(batch *commonProto.TxBatch) {
	tp.sendC.BatchedChan <- batch
}

//===================================
//       check txPool full
//===================================
func (tp *txPoolImpl) checkSpace() {
	if len(tp.batchStore)*tp.batchSize+len(tp.pendingTxs.txList) > tp.poolSize {
		atomic.StoreUint32(&tp.isFull, 1)
	} else {
		atomic.StoreUint32(&tp.isFull, 0)
	}
}